<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Training-Free Personalization via Retrieval and Reasoning on Fingerprints</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Georgia', serif;
            line-height: 1.6;
            color: #333;
            background-color: #fdfdfd;
            max-width: 1000px;
            margin: 0 auto;
            padding: 40px 20px;
        }

        .back-link {
            margin-bottom: 30px;
        }

        .back-link a {
            color: #3498db;
            text-decoration: none;
            font-size: 1.1em;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.3s ease;
        }

        .back-link a:hover {
            color: #2980b9;
        }

        .paper-header {
            text-align: center;
            margin-bottom: 50px;
            border-bottom: 2px solid #ecf0f1;
            padding-bottom: 30px;
        }

        .paper-title {
            font-size: 2.2em;
            font-weight: 400;
            color: #2c3e50;
            margin-bottom: 25px;
            line-height: 1.3;
        }

        .authors {
            font-size: 1.2em;
            color: #7f8c8d;
            margin-bottom: 15px;
            line-height: 1.5;
        }

        .author {
            display: inline;
        }

        .affiliations {
            font-size: 1em;
            color: #95a5a6;
            font-style: italic;
            margin-bottom: 25px;
        }

        .venue {
            background: linear-gradient(135deg, #e74c3c, #c0392b);
            color: white;
            padding: 8px 20px;
            border-radius: 25px;
            font-weight: 600;
            font-size: 1.1em;
            display: inline-block;
            margin-bottom: 25px;
        }

        .paper-links {
            display: flex;
            justify-content: center;
            gap: 15px;
            flex-wrap: wrap;
        }

        .paper-links a {
            color: white;
            text-decoration: none;
            padding: 12px 24px;
            border-radius: 6px;
            font-weight: 600;
            transition: all 0.3s ease;
            text-transform: uppercase;
            font-size: 0.9em;
            letter-spacing: 0.5px;
        }

        .pdf-link { background: #e74c3c; }
        .code-link { background: #27ae60; }
        .arxiv-link { background: #8e44ad; }

        .paper-links a:hover {
            transform: translateY(-2px);
            box-shadow: 0 6px 12px rgba(0,0,0,0.2);
        }

        .section {
            margin-bottom: 50px;
        }

        .section h2 {
            font-size: 1.8em;
            font-weight: 400;
            color: #2c3e50;
            margin-bottom: 25px;
            border-bottom: 2px solid #ecf0f1;
            padding-bottom: 10px;
        }

        .abstract-text {
            font-size: 1.1em;
            line-height: 1.8;
            text-align: justify;
            background-color: #f8f9fa;
            padding: 30px;
            border-radius: 8px;
            border-left: 5px solid #3498db;
        }

        .key-contributions {
            background-color: #e8f5e8;
            padding: 25px;
            border-radius: 8px;
            border-left: 5px solid #27ae60;
        }

        .key-contributions ul {
            list-style: none;
            padding-left: 0;
        }

        .key-contributions li {
            margin-bottom: 12px;
            padding-left: 25px;
            position: relative;
            font-size: 1.05em;
        }

        .key-contributions li:before {
            content: "‚úì";
            position: absolute;
            left: 0;
            color: #27ae60;
            font-weight: bold;
            font-size: 1.2em;
        }

        .figure-container {
            text-align: center;
            margin: 40px 0;
            background-color: #f9f9f9;
            padding: 30px;
            border-radius: 10px;
            border: 1px solid #e0e0e0;
        }

        .figure-container img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            box-shadow: 0 4px 12px rgba(0,0,0,0.1);
            margin-bottom: 15px;
        }

        .figure-caption {
            font-size: 0.95em;
            color: #666;
            font-style: italic;
            margin-top: 10px;
            line-height: 1.5;
        }

        .method-overview {
            background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
            padding: 30px;
            border-radius: 10px;
            margin: 30px 0;
        }

        .method-overview h3 {
            color: #2c3e50;
            margin-bottom: 15px;
            font-size: 1.3em;
        }

        .citation-box {
            background-color: #2c3e50;
            color: #ecf0f1;
            padding: 20px;
            border-radius: 6px;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
            line-height: 1.4;
            margin: 25px 0;
            position: relative;
            overflow-x: auto;
        }

        .copy-btn {
            position: absolute;
            top: 10px;
            right: 15px;
            background: #3498db;
            color: white;
            border: none;
            padding: 5px 10px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 0.8em;
        }

        .copy-btn:hover {
            background: #2980b9;
        }

        .dataset-highlight {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 25px;
            border-radius: 10px;
            margin: 30px 0;
        }

        .dataset-highlight h3 {
            margin-bottom: 15px;
            font-size: 1.3em;
        }

        @media (max-width: 768px) {
            body {
                padding: 20px 15px;
            }
            
            .paper-title {
                font-size: 1.8em;
            }
            
            .paper-links {
                flex-direction: column;
                align-items: center;
            }
            
            .paper-links a {
                width: 200px;
                text-align: center;
            }
        }
    </style>
</head>
<body>
    <div class="back-link">
        <a href="../index.html">‚Üê Back to Homepage</a>
    </div>

    <div class="paper-header">
        <h1 class="paper-title">Training-Free Personalization via Retrieval and Reasoning on Fingerprints</h1>
        
        <div class="authors">
            <span class="author">Deepayan Das</span><sup>1</sup>, 
            <span class="author">Davide Talon</span><sup>2</sup>, 
            <span class="author">Yiming Wang</span><sup>2</sup>, 
            <span class="author">Massimiliano Mancini</span><sup>1</sup>, 
            <span class="author">Elisa Ricci</span><sup>1,2</sup>
        </div>
        
        <div class="affiliations">
            <sup>1</sup>University of Trento &nbsp;&nbsp; <sup>2</sup>Fondazione Bruno Kessler
        </div>
        
        <!-- <div class="venue">ICCV 2025</div> -->
        
        <div class="paper-links">
            <a href="https://arxiv.org/abs/2503.18623" target="_blank" class="arxiv-link">ArXiv</a>
            <a href="#" class="code-link">Code (Coming Soon)</a>
        </div>
    </div>

    <div class="section">
        <h2>Abstract</h2>
        <div class="abstract-text">
            Vision Language Models (VLMs) have lead to major improvements in multimodal reasoning, yet they still struggle to understand user-specific concepts. Existing personalization methods address this limitation but heavily rely on training procedures, that can be either costly or unpleasant to individual users. We depart from existing work, and for the first time explore the training-free setting in the context of personalization. We propose a novel method, <strong>Retrieval and Reasoning for Personalization (R2P)</strong>, leveraging internal knowledge of VLMs. First, we leverage VLMs to extract the concept fingerprint, i.e., key attributes uniquely defining the concept within its semantic class. When a query arrives, the most similar fingerprints are retrieved and scored via chain-of-thought-reasoning. To reduce the risk of hallucinations, the scores are validated through cross-modal verification at the attribute level: in case of a discrepancy between the scores, R2P refines the concept association via pairwise multimodal matching, where the retrieved fingerprints and their images are directly compared with the query. We validate R2P on two publicly available benchmarks and a newly introduced dataset, Personal Concepts with Visual Ambiguity (PerVA), for concept identification highlighting challenges in visual ambiguity. R2P consistently outperforms state-of-the-art approaches on various downstream tasks across all benchmarks.
        </div>
    </div>

    <div class="section">
        <h2>Key Contributions</h2>
        <div class="key-contributions">
            <ul>
                <li>First work to explore <strong>training-free personalization</strong> for Vision Language Models</li>
                <li>Novel <strong>Retrieval and Reasoning for Personalization (R2P)</strong> method using concept fingerprints</li>
                <li>Introduction of <strong>PerVA dataset</strong> - Personal Concepts with Visual Ambiguity</li>
                <li>Cross-modal verification and pairwise matching to reduce hallucinations</li>
                <li>State-of-the-art performance across multiple benchmarks without any training</li>
            </ul>
        </div>
    </div>

    <div class="section">
        <h2>Method Overview</h2>
        <div class="method-overview">
            <h3>R2P: Retrieval and Reasoning for Personalization</h3>
            <p>Our approach operates in two main phases: <strong>(1) Personal database creation</strong> where we extract concept fingerprints using VLM-based VQA, and <strong>(2) Concept inference</strong> with retrieval-reasoning that uses attribute-focused Chain-of-Thought reasoning and cross-modal verification to accurately identify personalized concepts in query images.</p>
        </div>
        
        <div class="figure-container">
            <img src="../assets/images/pipeline.png" alt="Main Pipeline">
            <div class="figure-caption">
                <strong>Figure 1:</strong> Overview of our R2P method showing the two-phase approach: personal database creation and concept inference with retrieval-reasoning.
            </div>
        </div>
    </div>

    <div class="section">
        <h2>PerVA Dataset</h2>
        <div class="dataset-highlight">
            <h3>üéØ Personal Concepts with Visual Ambiguity (PerVA)</h3>
            <p>We introduce PerVA, a new benchmark specifically designed to evaluate personalization methods on visually ambiguous concepts. This dataset highlights the challenges in distinguishing between similar-looking personal items and concepts.</p>
        </div>
        
        <div class="figure-container">
            <img src="../assets/images/dataset-personalization-1.png" alt="PerVA Dataset">
            <div class="figure-caption">
                <strong>Figure 2:</strong> Examples from our PerVA dataset showing personal concepts with visual ambiguity across different categories.
            </div>
        </div>
    </div>

    <div class="section">
        <h2>Qualitative Results</h2>
        <div class="figure-container">
            <img src="../assets/images/qual_result-1.png" alt="Qualitative Results">
            <div class="figure-caption">
                <strong>Figure 3:</strong> Qualitative results showing our method's ability to correctly identify personalized concepts through retrieval and reasoning, with examples of concept inference and cross-modal verification.
            </div>
        </div>
    </div>

    <div class="section">
        <h2>Citation</h2>
        <div class="citation-box">
            <button class="copy-btn" onclick="copyToClipboard()">Copy</button>
            <pre id="citation-text">@article{das2025training,
  title={Training-Free Personalization via Retrieval and Reasoning on Fingerprints},
  author={Das, Deepayan and Talon, Davide and Wang, Yiming and Mancini, Massimiliano and Ricci, Elisa},
  journal={arXiv preprint arXiv:2503.18623},
  year={2025}
}</pre>
        </div>
    </div>

    <script>
        function copyToClipboard() {
            const citationText = document.getElementById('citation-text').textContent;
            navigator.clipboard.writeText(citationText).then(function() {
                const btn = document.querySelector('.copy-btn');
                const originalText = btn.textContent;
                btn.textContent = 'Copied!';
                btn.style.background = '#27ae60';
                setTimeout(() => {
                    btn.textContent = originalText;
                    btn.style.background = '#3498db';
                }, 2000);
            });
        }
    </script>
</body>
</html>